/storage/weinig/anaconda3/envs/cdl/lib/python3.10/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `PYTORCH_PRETRAINED_BERT_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/storage/weinig/anaconda3/envs/cdl/lib/python3.10/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `PYTORCH_TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/storage/weinig/anaconda3/envs/cdl/lib/python3.10/site-packages/transformers/utils/hub.py:106: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/storage/weinig/cdl-inflation-narrative/annotation/labelstudio/agreement.py:219: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train['label'] = train['label'].replace(self.label2id_map)
/storage/weinig/cdl-inflation-narrative/annotation/labelstudio/agreement.py:220: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  valid['label'] = valid['label'].replace(self.label2id_map)
/storage/weinig/cdl-inflation-narrative/annotation/labelstudio/agreement.py:221: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test['label'] = test['label'].replace(self.label2id_map)
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
annotator 5: 0 document(s) to annotate.
annotator 7: 0 document(s) to annotate.
annotator 8: 0 document(s) to annotate.
annotator 9: 0 document(s) to annotate.
inner_id_list 488, annotator_5: 488
Updated annotation from user 5, result saved in ./export/task_1_annotation.csv
Task 1 (User 5 - Sviatoslav)
inner_id_list 488, annotator_7: 488
Updated annotation from user 7, result saved in ./export/task_1_annotation.csv
Task 1 (User 6 - Julia)
inner_id_list 488, annotator_8: 488
Updated annotation from user 8, result saved in ./export/task_1_annotation.csv
Task 1 (User 7 - Darian)
inner_id_list 488, annotator_9: 488
Updated annotation from user 9, result saved in ./export/task_1_annotation.csv
Task 1 (User 8 - Kim)
has winner ratio: 0.8790983606557377
all agreeing count: 232
0.6858854912630578
Map:   0%|          | 0/341 [00:00<?, ? examples/s]Map: 100%|██████████| 341/341 [00:00<00:00, 569.56 examples/s]Map: 100%|██████████| 341/341 [00:00<00:00, 565.15 examples/s]
Map:   0%|          | 0/49 [00:00<?, ? examples/s]Map: 100%|██████████| 49/49 [00:00<00:00, 539.25 examples/s]
Map:   0%|          | 0/98 [00:00<?, ? examples/s]Map: 100%|██████████| 98/98 [00:00<00:00, 579.66 examples/s]Map: 100%|██████████| 98/98 [00:00<00:00, 576.19 examples/s]
Traceback (most recent call last):
  File "/storage/weinig/cdl-inflation-narrative/annotation/labelstudio/agreement.py", line 303, in <module>
    inflation_narrative.train_sequence_classifier()
  File "/storage/weinig/cdl-inflation-narrative/annotation/labelstudio/agreement.py", line 249, in train_sequence_classifier
    training_args = TrainingArguments(
  File "<string>", line 134, in __init__
  File "/storage/weinig/anaconda3/envs/cdl/lib/python3.10/site-packages/transformers/training_args.py", line 1791, in __post_init__
    self.device
  File "/storage/weinig/anaconda3/envs/cdl/lib/python3.10/site-packages/transformers/training_args.py", line 2313, in device
    return self._setup_devices
  File "/storage/weinig/anaconda3/envs/cdl/lib/python3.10/site-packages/transformers/utils/generic.py", line 62, in __get__
    cached = self.fget(obj)
  File "/storage/weinig/anaconda3/envs/cdl/lib/python3.10/site-packages/transformers/training_args.py", line 2186, in _setup_devices
    raise ImportError(
ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`
